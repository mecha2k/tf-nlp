import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os, re
from tqdm import tqdm

import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from transformers import BertTokenizer


epochs = 3
batch_size = 32
max_len = 39

tf.random.set_seed(42)
np.random.seed(42)

tokenizer = BertTokenizer.from_pretrained(
    "bert-base-multilingual-cased", cache_dir="../data/bert_ckpt", do_lower_case=False
)

test_sentence = "안녕하세요, 반갑습니다."
encode = tokenizer.encode(test_sentence)
token_print = [tokenizer.decode(token) for token in encode]
print(encode)
print(tokenizer.decode(encode))
print(token_print)

test_sentence = "Hello world !!"
encode = tokenizer.encode(test_sentence)
token_print = [tokenizer.decode(token) for token in encode]
print(encode)
print(tokenizer.decode(encode))
print(token_print)

print(tokenizer.all_special_tokens)
print(tokenizer.all_special_ids)


## Korean Movie Review Classification
train_data = pd.read_csv("../data/ratings_train.txt", header=0, delimiter="\t", quoting=3)
train_data = train_data.dropna()
train_data.head()


# https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.
# PreTrainedTokenizer.encode_plus
def bert_tokenizer(sent, MAX_LEN):
    encoded_dict = tokenizer.encode_plus(
        text=sent,
        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
        max_length=MAX_LEN,  # Pad & truncate all sentences.
        pad_to_max_length=True,
        return_attention_mask=True,  # Construct attn. masks.
    )

    input_id = encoded_dict["input_ids"]
    attention_mask = encoded_dict["attention_mask"]
    # And its attention mask (simply differentiates padding from non-padding).
    token_type_id = encoded_dict["token_type_ids"]  # differentiate two sentences

    return input_id, attention_mask, token_type_id


# train_data = train_data[:1000] # for test
#
# input_ids = []
# attention_masks = []
# token_type_ids = []
# train_data_labels = []
#
# for train_sent, train_label in tqdm(
#     zip(train_data["document"], train_data["label"]), total=len(train_data)
# ):
#     try:
#         input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)
#
#         input_ids.append(input_id)
#         attention_masks.append(attention_mask)
#         token_type_ids.append(token_type_id)
#         train_data_labels.append(train_label)
#
#     except Exception as e:
#         print(e)
#         print(train_sent)
#         pass
#
# train_movie_input_ids = np.array(input_ids, dtype=int)
# train_movie_attention_masks = np.array(attention_masks, dtype=int)
# train_movie_type_ids = np.array(token_type_ids, dtype=int)
# train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_type_ids)
#
# train_data_labels = np.asarray(train_data_labels, dtype=np.int32)  # 레이블 토크나이징 리스트
# print("# sents: {}, # labels: {}".format(len(train_movie_input_ids), len(train_data_labels)))
#
# # 최대 길이: 39
# input_id = train_movie_input_ids[1]
# attention_mask = train_movie_attention_masks[1]
# token_type_id = train_movie_type_ids[1]
#
# print(input_id)
# print(attention_mask)
# print(token_type_id)
# print(tokenizer.decode(input_id))
#
#
# class TFBertClassifier(tf.keras.Model):
#     def __init__(self, model_name, dir_path, num_class):
#         super(TFBertClassifier, self).__init__()
#
#         self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)
#         self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)
#         self.classifier = tf.keras.layers.Dense(
#             num_class,
#             kernel_initializer=tf.keras.initializers.TruncatedNormal(
#                 self.bert.config.initializer_range
#             ),
#             name="classifier",
#         )
#
#     def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):
#         # outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)
#         outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)
#         pooled_output = outputs[1]
#         pooled_output = self.dropout(pooled_output, training=training)
#         logits = self.classifier(pooled_output)
#
#         return logits
#
#
# cls_model = TFBertClassifier(
#     model_name="bert-base-multilingual-cased", dir_path="bert_ckpt", num_class=2
# )
#
# # 학습 준비하기
# optimizer = tf.keras.optimizers.Adam(3e-5)
# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
# metric = tf.keras.metrics.SparseCategoricalAccuracy("accuracy")
# cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])
#
# model_name = "tf2_bert_naver_movie"
#
# # overfitting을 막기 위한 ealrystop 추가
# earlystop_callback = EarlyStopping(monitor="val_accuracy", min_delta=0.0001, patience=2)
# # min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)
# # patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\
#
# checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, "weights.h5")
# checkpoint_dir = os.path.dirname(checkpoint_path)
#
# # Create path if exists
# if os.path.exists(checkpoint_dir):
#     print("{} -- Folder already exists \n".format(checkpoint_dir))
# else:
#     os.makedirs(checkpoint_dir, exist_ok=True)
#     print("{} -- Folder create complete \n".format(checkpoint_dir))
#
# cp_callback = ModelCheckpoint(
#     checkpoint_path, monitor="val_accuracy", verbose=1, save_best_only=True, save_weights_only=True
# )
#
# # 학습과 eval 시작
# history = cls_model.fit(
#     train_movie_inputs,
#     train_data_labels,
#     epochs=NUM_EPOCHS,
#     batch_size=BATCH_SIZE,
#     validation_split=VALID_SPLIT,
#     callbacks=[earlystop_callback, cp_callback],
# )
#
# # steps_for_epoch
# #
# def plot_graphs(history, string):
#     plt.plot(history.history[string])
#     plt.plot(history.history["val_" + string], "")
#     plt.xlabel("Epochs")
#     plt.ylabel(string)
#     plt.legend([string, "val_" + string])
#     plt.show()
# print(history.history)
# plot_graphs(history, "loss")
#
# # # Korean Movie Review Test 데이터
# test_data = pd.read_csv(DATA_TEST_PATH, header=0, delimiter="\t", quoting=3)
# test_data = test_data.dropna()
# test_data.head()
#
# input_ids = []
# attention_masks = []
# token_type_ids = []
# test_data_labels = []
#
# for test_sent, test_label in tqdm(zip(test_data["document"], test_data["label"])):
#     try:
#         input_id, attention_mask, token_type_id = bert_tokenizer(test_sent, MAX_LEN)
#
#         input_ids.append(input_id)
#         attention_masks.append(attention_mask)
#         token_type_ids.append(token_type_id)
#         test_data_labels.append(test_label)
#     except Exception as e:
#         print(e)
#         print(test_sent)
#         pass
#
# test_movie_input_ids = np.array(input_ids, dtype=int)
# test_movie_attention_masks = np.array(attention_masks, dtype=int)
# test_movie_type_ids = np.array(token_type_ids, dtype=int)
# test_movie_inputs = (test_movie_input_ids, test_movie_attention_masks, test_movie_type_ids)
#
# test_data_labels = np.asarray(test_data_labels, dtype=np.int32)  # 레이블 토크나이징 리스트
# print("num sents, labels {}, {}".format(len(test_movie_input_ids), len(test_data_labels)))
#
# results = cls_model.evaluate(test_movie_inputs, test_data_labels, batch_size=1024)
# print("test loss, test acc: ", results)
