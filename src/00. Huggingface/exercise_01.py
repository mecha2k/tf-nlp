from transformers import PreTrainedTokenizerFast

special_tokens = {
    "bos_token": "<s>",
    "eos_token": "</s>",
    "pad_token": "<pad>",
    "unk_token": "<unk>",
    "mask_token": "<mask>",
}

tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", **special_tokens)

bos_token = "<bos>"
question = ["ì•ˆë…•", "í•˜", "ì„¸", "ìš”"]
question = " ".join(question)
print(question)
answer = "ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ìž…ë‹ˆë‹¤.ðŸ˜¤:)"
inputs = bos_token + str(question) + str(answer)
print(inputs)
inputs = tokenizer.encode(inputs, return_tensors="pt")
print(inputs)
print(tokenizer.decode(inputs[0]))
